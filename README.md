This is the repository behind my Stoic Chat Bot, which has been trained on GPT-2. To boil down the essence of this project, I followed essentially the typical framework of creating an AI-chatbot: (1) gather the source data from which I was going to train the model, (2) preprocess the data, (3) and then training the data, (4) creating a simple user interface. I will describe each of these steps in detail so that other reseachers can replicate a similar project in the future.

Firstly, to describe why I chose to use GPT-2 instead of other LLMs is for the following: GPT-3 and OpenAI have recently been causing issues with accepting payments for their API service. Furthermore, for a smaller project like this, the cost component of using GPT-3 isn't as valuable as just documenting the entire process of creating a chat bot, as using GPT-3 would work exactly the way I have structured my code right now, with the same flow but just a different model to train on.

For the first step of this project, I needed to gather the source data from which I was going to train the model. Since I was creating a Stoic Chat bot, I web scraped two key stoic books: Discourses and Meditations. I found both of these from Project Gutenberg, which is an extremely useful online library that digitizes a lot of key philsophical texts. I then generated two texts files, *discourses.txt* and *meditations.txt* to contain the scraped contents of these scripts. The script *scraper.py* is the code I used to scrape these two books.

Secondly, I then preprocessed the text files using the script *preprocess.py* so that it could be accurately fed into the GPT-2 model. This step invovled a lot of tokenization of the data, since that is the way GPT-2 can understand the input data. I specifically broke up the text into sequences of 1024 characters (tokens) each, and then sequentially defined three more texts files: a train_file, *book_name_train.txt*, a value file, *book_name_val.txt*, and a test_file *book_name_test.txt". These three files are the key input components for training the model in the next step.

Thirdy, I finally trained the model from the tokenized files as mentioned before. This is done in the *train.py* script, which takes these tokenized files and then trains the model. On average, training the model on a big text file like *Meditations* and *Discourses* took me around 45 minutes per file. Because GitHub can't accept 100 mb or larger files, I can't actually push my model into my repo (a problem I'll talk about later).

Furthermore, after I finished training the model on the input files, I proceeded to fine-tune the model. All this involved was to select a smaller subset of the text file that I was training, for example two chapters of the meditations, and then repeating steps (2) and (3) above onto that file. The subsequent fine tuning training files are all named *book_name_ft* in my repository. This process was critical in ensuring that I had a better trained model and it helped improved the responses in my model quite substantially.

Finally, I hosted my application on Streamlit, which offered a clean interface for a chat-bot style application. I specifically used a library called st-chat which has a lot of the user interfacing that a chat bot usually has, and offered a convenient way to call functions to display the messages. However, I also ran into issues with trying to deploy my application into the internet, which I will talk about soon.

Overall, it was through this framework that I was able to create this application, and I believe that the same framework can be used to create a chat bot for any research purpose in the future. More importantly, it's also important to cover some of the biggest challenges that I encountered while building out this application, which included:
1. I could not push my model onto GitHub because it was too big. GitHub has an inherent limit of 100 mb per file on their platform. You could use a system like GIT LFS to push larger files, but I tried doing this and was unsuccessful in integrating it into my project, as even this said that I already ran out of free GIT LFS credits while I was attempting to push my model. For reference, the model size of my GPT-2 model was around 452 mb. I tried working around this by using other smaller LLMs like Albert, which I trained files on as well, but this same result still ended up happening. Future research could go into figuring out how to effectively compress the model to make it less than 100 mb so that it can be pushed onto the repository. Furthermore, other services like GitLab could be used to host larger files (albeit at a cost).
2. Because I couldn't compress my model size, or figure out a way to push a larger file into my repository, deploying my application to the Internet became very hard. This is because of the way that Streamlit works: Streamlit hooks itself into the GitHub repository of a project and then dynamically updates the deployed website with any new push changes into that repository. Because I couldn't push the changes of my model onto repository, I couldn't deploy onto Streamlit, as the responses in my chat bot are all generated by the model. This is a fundamental problem in this current flow because it would essentially mean that it's very hard to deploy chat-based AI services because of their reliance on these large models to generate responses. There are several startups that are attempting to create these "AI builder platforms" like Steamship, because of this exact issue, but development is still in the early stages and integration into projects like this is still difficult.

Thus, when conducting further research along this topic, I would urge more exploration into model size reduction, with particular respect to getting it less than 100 mb so that it can be deployed to GitHub and then subsequently hosted on Streamlit or similar hosting providers so that people can easily access these exciting AI applications. Please reach out to me if you have any further questions.